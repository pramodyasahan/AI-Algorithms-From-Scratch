---

# **AI-Algorithm-From-Scratch**

Welcome to **AI-Algorithm-From-Scratch**, a personal journey and comprehensive collection of **machine learning** and *
*deep learning** algorithms, built entirely from scratch. This repository is a testament to mastering the core
fundamentals and pushing the boundaries of artificial intelligence through hands-on implementation of cutting-edge
architectures and research papers.

> **Why This Repository?**
> - **Deep Learning** isn't just about frameworks like TensorFlow or PyTorchâ€”it's about understanding the core
    mechanisms, building models from the ground up, and truly mastering the underlying principles.
> - In this repository, I implement foundational algorithms, architectures, and research papers **from scratch** to not
    only understand them but to empower myself with the skills needed to become a **top-tier machine learning expert**.

---

## **ðŸš€ Repository Overview**

This repository is divided into several key sections that map directly to my learning path. Whether you're here to
explore classical machine learning algorithms, dive deep into neural network architectures, or learn from cutting-edge
research papers, you'll find everything organized clearly for ease of use and learning.

### **Key Sections:**

- [Supervised Learning Algorithms](#supervised-learning-algorithms)
- [Unsupervised Learning Algorithms](#unsupervised-learning-algorithms)
- [Optimization Techniques](#optimization-techniques)
- [Regularization Techniques](#regularization-techniques)
- [Deep Learning Architectures (Scratch)](#deep-learning-architectures-from-scratch)
- [Deep Learning Architectures (PyTorch)](#deep-learning-architectures-using-pytorch)
- [Research Paper Implementations](#research-paper-implementations)

---

## **ðŸ” Supervised Learning Algorithms**

These algorithms are implemented **from scratch**, using basic libraries like **NumPy** to build foundational supervised
models. This includes classification, regression, and ensemble techniques.

### **Implemented Algorithms:**

- **Linear Regression**: Gradient Descent and Normal Equation approaches.
- **Logistic Regression**: Binary and Multiclass classification models.
- **K-Nearest Neighbors (KNN)**: Distance-based classifier.
- **Support Vector Machine (SVM)**: Hinge loss optimization with both linear and kernelized approaches.
- **Decision Trees**: Recursive partitioning, information gain, and Gini impurity.
- **Random Forest**: Ensemble of decision trees using bagging.
- **AdaBoost**: Boosting weak learners with weighted samples.

[View Supervised Learning Folder](./algorithms/supervised_learning)

---

## **ðŸ” Unsupervised Learning Algorithms**

Discover the power of unsupervised learning techniques like clustering and dimensionality reduction, built from scratch
without using ML libraries.

### **Implemented Algorithms:**

- **K-Means Clustering**: Iterative centroid-based clustering.
- **Principal Component Analysis (PCA)**: Dimensionality reduction and feature extraction.
- **Hierarchical Clustering**: Agglomerative and divisive clustering.
- **Gaussian Mixture Model (GMM)**: Clustering using probabilistic models.
- **DBSCAN**: Density-based spatial clustering for discovering clusters in data.

[View Unsupervised Learning Folder](./algorithms/unsupervised_learning)

---

## **ðŸ”§ Optimization Techniques**

Optimization is the heartbeat of machine learning. These algorithms are implemented from scratch to understand how *
*gradient-based optimization** works.

### **Included Techniques:**

- **Gradient Descent**: Simple, batch, and mini-batch implementations.
- **Stochastic Gradient Descent (SGD)**: Training with data in smaller batches.
- **Adam Optimizer**: Adaptive learning rates and momentum.
- **RMSProp**: Root Mean Square Propagation for adaptive learning.
- **Nesterov Accelerated Gradient**: Faster convergence using momentum.

[View Optimization Folder](./optimization)

---

## **ðŸ›¡ï¸ Regularization Techniques**

Regularization methods prevent overfitting and improve model generalization. Hereâ€™s a set of techniques, implemented
from scratch, to build more robust models.

### **Included Techniques:**

- **L1 Regularization**: Feature selection with Lasso regression.
- **L2 Regularization**: Ridge regression to shrink coefficients.
- **Dropout**: Randomly dropping neurons to prevent overfitting.
- **Batch Normalization**: Standardizing layer outputs to accelerate training.
- **Early Stopping**: Stopping training when validation performance decreases.

[View Regularization Folder](./regularization)

---

## **ðŸ’¡ Deep Learning Architectures (From Scratch)**

Implementing deep learning architectures **from scratch** is the ultimate challenge and key to understanding how neural
networks really work.

### **Included Architectures:**

- **Convolutional Neural Networks (CNNs)**: Implemented for image classification.
- **Recurrent Neural Networks (RNNs)**: Sequence modeling using time-step recurrence.
- **Long Short-Term Memory (LSTM)**: Overcoming vanishing gradient problems in RNNs.
- **Generative Adversarial Networks (GANs)**: Building both generator and discriminator models from scratch.
- **Transformers**: Multi-head attention and self-attention for sequence modeling.

[View Architectures Folder (From Scratch)](./architectures)

---

## **ðŸ’¡ Deep Learning Architectures (Using PyTorch)**

While scratch implementations help in understanding, **PyTorch** is used to scale and build complex architectures more
efficiently. Here, youâ€™ll find advanced models implemented using **PyTorch**.

### **Included Architectures:**

- **Convolutional Neural Networks (CNNs)**: Efficient training with PyTorchâ€™s autograd.
- **Recurrent Neural Networks (RNNs)**: Sequence modeling with PyTorch.
- **LSTMs and GRUs**: Handling sequential data and long-term dependencies.
- **Generative Adversarial Networks (GANs)**: GAN variations with PyTorch.
- **Transformers**: Attention-based architectures for NLP and vision tasks.

[View PyTorch Architectures Folder](./pytorch_architectures)

---

## **ðŸ”¬ Research Paper Implementations**

A special section where I tackle implementations of **cutting-edge research papers**. These implementations combine the
knowledge gained from scratch implementations and PyTorch efficiency.

### **Key Papers Implemented:**

- **"Attention is All You Need"**: Implementing Transformers from the groundbreaking paper.
- **"U-Net: Convolutional Networks for Biomedical Image Segmentation"**: Deep learning for pixel-wise classification.
- **"Generative Adversarial Networks"**: The original GAN paper implemented.
- **"BERT: Pre-training of Deep Bidirectional Transformers"**: Understanding transformers for NLP.
- **"StyleGAN: A Style-Based Generator Architecture for GANs"**: High-resolution image generation.

[View Research Paper Folder](./research_papers)

---

## **âš™ï¸ How to Use This Repository**

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/pramodyasahan/AI-Algorithms-From-Scratch.git
   cd AI-Algorithm-FromScratch
   ```

2. **Navigate to Specific Folders**: Each folder contains a `README.md` explaining the algorithm/architecture, its
   theory, and how to run the code.

3. **Run Example Notebooks**: Where applicable, Jupyter notebooks are provided to showcase example use cases and
   performance of each algorithm/model.

---

## **ðŸŒŸ Vision for This Repository**

This repository is not just a learning toolâ€”it's a **showcase** of whatâ€™s possible when you commit to mastering the core
fundamentals of machine learning and deep learning. By understanding how models work from scratch, you build the
foundation needed to innovate, solve real-world problems, and push the boundaries of AI.

**Future Goals**:

- Add **more cutting-edge research paper implementations**.
- Optimize algorithms and models for **scalability**.
- Document each step of the learning journey through **blogs** and **tutorials**.

---

## **ðŸš€ Let's Connect!**

If you find this repository helpful or want to collaborate on exciting ML projects, feel free to reach out:

- **LinkedIn**: [pramodyasahan](https://www.linkedin.com/in/pramodyasahan/)
- **Email**: [pramodyasahan.edu@gmail.com](mailto:pramodyasahan.edu@gmail.com)

---

> **"The best way to learn AI is by building it from scratch. This repository is my journey, and I hope it inspires
others to start theirs."**

---

## **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
